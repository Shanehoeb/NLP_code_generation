# NLP_code_generation
Final Project for the Natural Language Processing course.

<img width="692" alt="Capture d’écran 2023-06-20 à 03 45 32" src="https://github.com/Shanehoeb/NLP_code_generation/assets/88781950/f98faff7-5b50-4f90-aefd-632260e5db1a">

A lot of recent work on both Large Language Models (LLMs) and generative models for text showed particular improvement on the code generation task and reaffirmed the higher importance of this task. 
Inspired by the paper https://github.com/openai/human-eval and the performance of the associated model, Codex, we decide to focus on the code generation task and to discuss the convergence between code generation models and "human-style" logic and development concepts in the process of code generation.

First, we reproduce some important results about Large Language models and their ability not only to perform very well on zero-shot, but moreover to improve substantially with fine-tuning on code. Then, we try to explore potential enhancements of the LLM state of the art models by making a step towards more human-like reasoning. We also explore an alternative in the recent breakthrough in generative AI that are diffusion models. Finally, we discuss further models and techniques for aiming at more convergence towards State of the art code generation models and human-like programming logic.

The report is available [here](https://github.com/https://github.com/Shanehoeb/NLP_code_generation/report.pdf)

The github is organized in 3 parts :
- 1 for reproducing the rsults -> lien
- 1 for codeT exploration
- 1 for the latent-diffusion for text exploration

Mot de remerciement

$schema: http://azureml/sdk-2-0/CommandComponent.json
name: microsoft.msra.dki.verifier_trainer
display_name: Verifier Train
version: 0.1.2-dev1
is_deterministic: True
type: CommandComponent
description: Verifier Train
tags: {category: Verifier Training, contact: Zeqi.Lin@microsoft.com}
inputs:
  wandb_run_name:
    type: string
    description: A friendly displayed name on wandb.ai for this run
    default: GSM8K-5demos-1000examples-alpha0
  dataset_name:
    type: enum
    description: Name of the dataset to be run. GSM8K/CLUTRR/strategyQA
    default: GSM8K
    enum:
      - GSM8K
      - CLUTRR
      - strategyQA
  train_data:
    type: path
    description: train.txt
  test_data:
    type: path
    description: test.txt
  previous_run_dir:
    type: path
    description: previous_run_dir
    optional: true
  previous_run_epoch:
    type: integer
    description: previous_run_epoch
    optional: true
    default: 1
  model_name_or_path:
    type: string
    description: model_name_or_path
    default: microsoft/deberta-v3-large
  save_strategy:
    type: string
    desacription: save_strategy
    default: epoch
  evaluation_strategy:
    type: string
    desacription: evaluation_strategy
    default: epoch
  learning_rate:
    type: number
    description: Learning rate, default is 1e-5
    default: 1e-5
  per_device_batch_size:
    type: integer
    description: per_device_batch_size
    default: 8
  seed:
    type: integer
    description: Random seed, default is 1
    default: 1
  do_train:
    type: boolean
    description: True or False
    default: True
  do_eval:
    type: boolean
    description: True or False
    default: True
  alpha:
    type: number
    description: The loss weight of stepwise labels, default is 0.0
    default: 0.0
  num_train_epochs:
    type: integer
    description: default is 5
    default: 5
outputs:
  output_dir:
    type: path
    optional: false
    description: The path of the output
environment:
  docker:
    image: mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220516.v1
  os: Linux
  conda:
    conda_dependencies:
      name: project_environment
      channels:
      - defaults
      - pytorch
      dependencies:
      - python=3.8
      - pip=20.0
      - pip:
        - torch==1.7.0+cu110
        - -f https://download.pytorch.org/whl/torch_stable.html
        - wandb==0.12.7
        - future
        - numpy==1.20.3
        - transformers==4.6.0
        - datasets==1.11.0
        - huggingface-hub==0.0.8
        - nltk
        - rouge-score
        - sacrebleu==1.5.1
        - sentencepiece
        - sklearn
        - deepspeed
        - seqeval
        - conllu
        - multiset
        - azureml-sdk
        - azureml-dataset-runtime
successful_return_code: Zero
meta:
  requireGpu: True
command: >-
  export WANDB_API_KEY={your_wandb_api_key} &&
  export WANDB_PROJECT=deberta-verifier &&
  export WANDB_RUN_ID={inputs.wandb_run_name} &&
  export WANDB_TAGS=deberta_verifier &&
  export NCCL_DEBUG=INFO &&
  cd src &&
  deepspeed --num_gpus=8
  run_ner.py
  --task_type NER
  --dataset_name {inputs.dataset_name}
  --train_data {inputs.train_data}
  --test_data {inputs.test_data}
  [--previous_run_dir {inputs.previous_run_dir}]
  [--previous_run_epoch {inputs.previous_run_epoch}]
  --model_name_or_path {inputs.model_name_or_path}
  --output_dir {outputs.output_dir}
  --max_seq_length 512
  --per_device_train_batch_size {inputs.per_device_batch_size}
  --per_device_eval_batch_size 64
  --save_strategy {inputs.save_strategy}
  --evaluation_strategy {inputs.evaluation_strategy}
  --learning_rate {inputs.learning_rate}
  --lr_scheduler_type constant
  --seed {inputs.seed}
  --do_train {inputs.do_train}
  --do_eval {inputs.do_eval}
  --num_train_epochs {inputs.num_train_epochs}
  --logging_steps 10
  --overwrite_output_dir
  --alpha {inputs.alpha}
  --deepspeed ds_config.json
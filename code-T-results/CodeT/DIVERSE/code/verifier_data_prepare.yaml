$schema: http://azureml/sdk-2-0/CommandComponent.json
name: microsoft.msra.dki.verifier_data_preparing
display_name: Verifier Data Preparing
version: 0.1.8-dev2
type: CommandComponent
is_deterministic: true
description: Verifier Data Preparing
tags: {category: Verifier Data Preparing, contact: Zeqi.Lin@microsoft.com}
inputs:
  generator_result_file:
    type: path
    optional: false
    description: The generation result file generated by GPT-3 onebox or other modules
  random_seed:
    type: integer
    description: Random seed
    default: 233
  split:
    type: enum
    description: train / dev. randomly shuffle train dataset.
    default: train
    enum:
      - train
      - dev
  dataset_name:
    type: enum
    description: Name of the dataset to be run. GSM8K/CLUTRR/strategyQA
    default: GSM8K
    enum:
      - GSM8K
      - CLUTRR
      - strategyQA
  text_entailment_model_name:
    type: string
    description: The text entailment model that is used in step labeling, such as roberta-large-mnli, facebook/bart-large-mnli, etc.
    default: roberta-large-mnli
  text_entailment_batch_size:
    type: number
    description: text entailment batch size
    default: 256
outputs:
    output_dir:
      type: path
      optional: false
      description: The output dir that you want to save the output data, which is the verifier training module's input.
environment:
  docker:
    image: mcr.microsoft.com/azureml/pytorch-1.9-ubuntu18.04-py37-cuda11.0.3-gpu-inference:20220516.v3
  os: Linux
  conda:
    conda_dependencies:
      name: project_environment
      channels:
      - defaults
      - pytorch
      dependencies:
      - python=3.7
      - pip=20.0
      - pip:
        - torch==1.7.0+cu110
        - -f https://download.pytorch.org/whl/torch_stable.html
        - multiset
        - tqdm
        - nltk
        - transformers==4.6.0
        - datasets==1.11.0
        - huggingface-hub==0.0.8
successful_return_code: Zero
meta:
  requireGpu: False
command: >-
  cd src &&
  python verifier_data_prepare.py
  --generator_result_file {inputs.generator_result_file}
  --output_dir {outputs.output_dir}
  --split {inputs.split}
  --random_seed {inputs.random_seed}
  --dataset_name {inputs.dataset_name}
  --text_entailment_model_name {inputs.text_entailment_model_name}
  --text_entailment_batch_size {inputs.text_entailment_batch_size}
